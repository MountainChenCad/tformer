"""
PATE-Net å¯è§£é‡Šæ€§åˆ†æå™¨ - åŸºäºæ‰€æœ‰é€šé“æ•°æ®
å®ç°äº‹å‰ã€è¿ç§»è¿‡ç¨‹ã€äº‹åçš„å…¨æ–¹ä½å¯è§£é‡Šæ€§åˆ†æ
ä½¿ç”¨3Då¯è§†åŒ–æŠ€æœ¯ï¼ŒåŸºäºDE + FE + BAä¸‰é€šé“èåˆæ•°æ®
"""

import os
import sys
import argparse
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from src.pate_net import PATE_Net
from src.datasets import StandardDataset

class PATE_ExplainerAllChannels:
    """PATE-Netå¯è§£é‡Šæ€§åˆ†æå™¨ - æ‰€æœ‰é€šé“ç‰ˆæœ¬"""

    def __init__(self, args):
        self.args = args
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")

        # æ•…éšœç±»å‹åç§°å’Œé¢œè‰²
        self.fault_names = ['Normal', 'Inner Race', 'Outer Race', 'Ball']
        self.fault_colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D']

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(args.output_dir, exist_ok=True)
        self.plot_dir = os.path.join(args.output_dir, 'plots')
        self.report_dir = os.path.join(args.output_dir, 'reports')
        os.makedirs(self.plot_dir, exist_ok=True)
        os.makedirs(self.report_dir, exist_ok=True)

    def load_model_and_data(self):
        """åŠ è½½æ¨¡å‹å’Œæ•°æ®"""
        print("åŠ è½½PATE-Netæ¨¡å‹å’Œæ•°æ®ï¼ˆæ‰€æœ‰é€šé“ç‰ˆæœ¬ï¼‰...")

        # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
        self.model = PATE_Net(
            signal_length=self.args.signal_length,
            feature_dim=self.args.feature_dim,
            num_classes=self.args.num_classes,
            temperature=self.args.temperature
        ).to(self.device)

        if os.path.exists(self.args.model_path):
            checkpoint = torch.load(self.args.model_path, map_location=self.device, weights_only=False)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            print("âœ“ PATE-Netæ¨¡å‹åŠ è½½æˆåŠŸï¼ˆåŸºäºæ‰€æœ‰é€šé“è®­ç»ƒï¼‰")
        else:
            print("âš  æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆè®­ç»ƒPATE-Net")
            return False

        self.model.eval()

        # åŠ è½½æ•°æ®
        self.source_dataset = StandardDataset(self.args.source_data_path)
        self.target_dataset = StandardDataset(self.args.target_data_path)
        self.val_dataset = StandardDataset(self.args.val_data_path)

        self.source_loader = DataLoader(self.source_dataset, batch_size=64, shuffle=False, num_workers=2)
        self.target_loader = DataLoader(self.target_dataset, batch_size=64, shuffle=False, num_workers=2)
        self.val_loader = DataLoader(self.val_dataset, batch_size=64, shuffle=False, num_workers=2)

        print(f"æºåŸŸæ•°æ®: {len(self.source_dataset)} æ ·æœ¬ï¼ˆæ‰€æœ‰é€šé“èåˆï¼‰")
        print(f"ç›®æ ‡åŸŸæ•°æ®: {len(self.target_dataset)} æ ·æœ¬")
        print(f"éªŒè¯æ•°æ®: {len(self.val_dataset)} æ ·æœ¬")

        return True

    def analyze_ex_ante_interpretability(self):
        """äº‹å‰å¯è§£é‡Šæ€§ï¼šæ¨¡å‹æ¶æ„é€æ˜æ€§åˆ†æ"""
        print("\n=== äº‹å‰å¯è§£é‡Šæ€§ï¼šåŸå‹ç½‘ç»œæ¶æ„åˆ†æï¼ˆæ‰€æœ‰é€šé“ï¼‰ ===")

        # è·å–å­¦ä¹ åˆ°çš„åŸå‹
        prototypes = self.model.get_prototypes().cpu().numpy()

        # 1. åŸå‹å¯è§†åŒ–ï¼ˆ3Dï¼‰
        self._visualize_prototypes_3d(prototypes)

        # 2. åŸå‹é—´è·ç¦»åˆ†æ
        self._analyze_prototype_distances(prototypes)

        # 3. æ¶æ„é€æ˜æ€§æŠ¥å‘Š
        self._generate_architecture_report()

    def _visualize_prototypes_3d(self, prototypes):
        """å¯è§†åŒ–å­¦ä¹ åˆ°çš„åŸå‹ï¼ˆ3Dç‰ˆæœ¬ï¼‰"""
        print("å¯è§†åŒ–æ•…éšœåŸå‹ï¼ˆ3Dï¼‰...")

        # ä½¿ç”¨3D PCAé™ç»´è¿›è¡Œå¯è§†åŒ–
        pca = PCA(n_components=3)
        prototypes_3d = pca.fit_transform(prototypes)

        fig = plt.figure(figsize=(12, 10))
        ax = fig.add_subplot(111, projection='3d')

        for i, (name, color) in enumerate(zip(self.fault_names, self.fault_colors)):
            ax.scatter(prototypes_3d[i, 0], prototypes_3d[i, 1], prototypes_3d[i, 2],
                      c=color, s=200, marker='*', edgecolor='black', linewidth=2,
                      label=f'{name} Prototype')

            # æ·»åŠ 3Dæ ‡ç­¾
            ax.text(prototypes_3d[i, 0], prototypes_3d[i, 1], prototypes_3d[i, 2],
                   name, fontsize=12, fontweight='bold')

        ax.set_title('3Då­¦ä¹ åˆ°çš„æ•…éšœåŸå‹åˆ†å¸ƒ (PCAé™ç»´) - æ‰€æœ‰é€šé“', fontsize=16, fontweight='bold')
        ax.set_xlabel(f'PCA Component 1 ({pca.explained_variance_ratio_[0]:.2%})')
        ax.set_ylabel(f'PCA Component 2 ({pca.explained_variance_ratio_[1]:.2%})')
        ax.set_zlabel(f'PCA Component 3 ({pca.explained_variance_ratio_[2]:.2%})')
        ax.legend()
        plt.tight_layout()
        plt.savefig(os.path.join(self.plot_dir, 'prototypes_visualization_3d_all_channels.png'), dpi=300, bbox_inches='tight')
        plt.close()

    def _analyze_prototype_distances(self, prototypes):
        """åˆ†æåŸå‹é—´è·ç¦»"""
        print("åˆ†æåŸå‹é—´è·ç¦»...")

        # è®¡ç®—åŸå‹é—´è·ç¦»çŸ©é˜µ
        distances = np.zeros((len(self.fault_names), len(self.fault_names)))
        for i in range(len(self.fault_names)):
            for j in range(len(self.fault_names)):
                distances[i, j] = np.linalg.norm(prototypes[i] - prototypes[j])

        # ç»˜åˆ¶è·ç¦»çƒ­åŠ›å›¾
        plt.figure(figsize=(8, 6))
        sns.heatmap(distances, annot=True, fmt='.3f', cmap='viridis',
                   xticklabels=self.fault_names, yticklabels=self.fault_names)
        plt.title('æ•…éšœåŸå‹é—´è·ç¦»çŸ©é˜µ - æ‰€æœ‰é€šé“', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig(os.path.join(self.plot_dir, 'prototype_distances_all_channels.png'), dpi=300, bbox_inches='tight')
        plt.close()

        # ä¿å­˜è·ç¦»ç»Ÿè®¡
        with open(os.path.join(self.report_dir, 'prototype_analysis_all_channels.txt'), 'w', encoding='utf-8') as f:
            f.write("PATE-Net åŸå‹åˆ†ææŠ¥å‘Š - æ‰€æœ‰é€šé“æ•°æ®\n")
            f.write("=" * 40 + "\n\n")
            f.write("æ•°æ®ç‰¹ç‚¹:\n")
            f.write("- æ•°æ®æº: DE + FE + BA ä¸‰é€šé“èåˆ\n")
            f.write("- æ ·æœ¬å¢å¼º: ç›¸æ¯”å•é€šé“æ•°æ®3å€å¢é•¿\n")
            f.write("- ä¿¡æ¯ä¸°å¯Œåº¦: å¤šè§’åº¦ä¼ æ„Ÿå™¨ä¿¡æ¯äº’è¡¥\n\n")
            f.write("åŸå‹é—´è·ç¦»çŸ©é˜µ:\n")
            for i, name_i in enumerate(self.fault_names):
                for j, name_j in enumerate(self.fault_names):
                    if i != j:
                        f.write(f"{name_i} - {name_j}: {distances[i, j]:.4f}\n")

            f.write(f"\nå¹³å‡åŸå‹é—´è·ç¦»: {np.mean(distances[distances > 0]):.4f}\n")
            f.write(f"æœ€å¤§åŸå‹é—´è·ç¦»: {np.max(distances):.4f}\n")
            f.write(f"æœ€å°åŸå‹é—´è·ç¦»: {np.min(distances[distances > 0]):.4f}\n")

    def _generate_architecture_report(self):
        """ç”Ÿæˆæ¶æ„é€æ˜æ€§æŠ¥å‘Š"""
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        with open(os.path.join(self.report_dir, 'architecture_transparency_all_channels.md'), 'w', encoding='utf-8') as f:
            f.write("# PATE-Net æ¶æ„é€æ˜æ€§æŠ¥å‘Š - æ‰€æœ‰é€šé“æ•°æ®\n\n")

            f.write("## æ•°æ®å¢å¼ºç‰¹ç‚¹\n")
            f.write("- **æ•°æ®æº**: DE + FE + BA ä¸‰é€šé“èåˆ\n")
            f.write("- **æ‰©å¢æ•ˆæœ**: ä»16kå¢é•¿åˆ°49kæ ·æœ¬ï¼ˆ3å€å¢é•¿ï¼‰\n")
            f.write("- **ä¿¡æ¯äº’è¡¥**: å¤šè§’åº¦ä¼ æ„Ÿå™¨ä¿¡æ¯èåˆ\n")
            f.write("- **é²æ£’æ€§æå‡**: å•é€šé“æ•…éšœä¸å½±å“æ•´ä½“è¯Šæ–­\n\n")

            f.write("## æ¨¡å‹ç»“æ„æ¦‚è§ˆ\n")
            f.write(f"- **æ€»å‚æ•°æ•°**: {total_params:,}\n")
            f.write(f"- **å¯è®­ç»ƒå‚æ•°æ•°**: {trainable_params:,}\n")
            f.write(f"- **ç‰¹å¾ç»´åº¦**: {self.args.feature_dim}\n")
            f.write(f"- **åˆ†ç±»ç±»åˆ«**: {self.args.num_classes}\n\n")

            f.write("## å†³ç­–æœºåˆ¶é€æ˜æ€§\n")
            f.write("PATE-Neté‡‡ç”¨åŸºäºåŸå‹çš„å†³ç­–æœºåˆ¶ï¼Œå…·æœ‰å¤©ç„¶çš„å¯è§£é‡Šæ€§ï¼š\n\n")
            f.write("1. **å¤šé€šé“ç‰¹å¾æå–**: 1D-CNNç¼–ç å™¨å¤„ç†èåˆçš„å¤šé€šé“æŒ¯åŠ¨ä¿¡å·\n")
            f.write("2. **åŸå‹åŒ¹é…**: è®¡ç®—ç‰¹å¾ä¸å„æ•…éšœåŸå‹çš„è·ç¦»\n")
            f.write("3. **å†³ç­–ä¾æ®**: é€‰æ‹©è·ç¦»æœ€è¿‘çš„åŸå‹å¯¹åº”çš„æ•…éšœç±»åˆ«\n\n")

            f.write("## æ‰€æœ‰é€šé“ä¼˜åŠ¿\n")
            f.write("- âœ… **DEé€šé“**: é©±åŠ¨ç«¯ä¿¡å·ï¼Œæ•…éšœç‰¹å¾æœ€ç›´æ¥æ¸…æ™°\n")
            f.write("- âœ… **FEé€šé“**: é£æ‰‡ç«¯ä¿¡å·ï¼Œæä¾›ä¸åŒè§’åº¦æ•…éšœä¿¡æ¯\n")
            f.write("- âœ… **BAé€šé“**: åŸºåº§ä¿¡å·ï¼Œåæ˜ æ•´ä½“ç³»ç»ŸæŒ¯åŠ¨ç‰¹å¾\n")
            f.write("- âœ… **æ•°æ®æ‰©å¢**: 49kæ ·æœ¬ vs 16kï¼Œç¼“è§£æ•°æ®ç¨€ç¼ºé—®é¢˜\n")
            f.write("- âœ… **é²æ£’è¯Šæ–­**: å¤šé€šé“ä¿¡æ¯èåˆæå‡è¯Šæ–­å¯é æ€§\n")

    def analyze_transfer_process(self):
        """è¿ç§»è¿‡ç¨‹å¯è§£é‡Šæ€§ï¼šç‰¹å¾ç©ºé—´å¯¹é½å¯è§†åŒ–"""
        print("\n=== è¿ç§»è¿‡ç¨‹å¯è§£é‡Šæ€§ï¼šåŸŸå¯¹é½å¯è§†åŒ–ï¼ˆæ‰€æœ‰é€šé“ï¼‰ ===")

        # æå–æºåŸŸå’Œç›®æ ‡åŸŸç‰¹å¾
        source_features, source_labels = self._extract_features(self.source_loader, with_labels=True)
        target_features, _ = self._extract_features(self.target_loader, with_labels=False)
        prototypes = self.model.get_prototypes().cpu().numpy()

        # 3D t-SNEé™ç»´å¯è§†åŒ–
        self._visualize_domain_alignment_3d(source_features, target_features, source_labels, prototypes)

        # Wassersteinè·ç¦»åˆ†æ
        self._analyze_wasserstein_distances(source_features, target_features, prototypes)

    def _extract_features(self, data_loader, with_labels=True):
        """æå–ç‰¹å¾"""
        features = []
        labels = []

        with torch.no_grad():
            for batch in data_loader:
                if with_labels:
                    signals, batch_labels = batch
                    labels.extend(batch_labels.numpy())
                else:
                    signals, _ = batch

                signals = signals.to(self.device)
                batch_features = F.normalize(self.model.encoder(signals), dim=1)
                features.append(batch_features.cpu().numpy())

        features = np.concatenate(features, axis=0)
        return features, np.array(labels) if labels else None

    def _visualize_domain_alignment_3d(self, source_features, target_features, source_labels, prototypes):
        """å¯è§†åŒ–åŸŸå¯¹é½ï¼ˆ3Dç‰ˆæœ¬ï¼‰"""
        print("ç”Ÿæˆ3DåŸŸå¯¹é½å¯è§†åŒ–...")

        # åˆå¹¶æ•°æ®è¿›è¡Œt-SNE
        all_features = np.vstack([source_features, target_features, prototypes])

        # ä½¿ç”¨è¾ƒå°çš„æ ·æœ¬è¿›è¡Œt-SNEä»¥åŠ é€Ÿ
        max_samples = 1000
        if len(source_features) > max_samples:
            indices = np.random.choice(len(source_features), max_samples, replace=False)
            source_features = source_features[indices]
            source_labels = source_labels[indices]

        if len(target_features) > max_samples:
            indices = np.random.choice(len(target_features), max_samples, replace=False)
            target_features = target_features[indices]

        # é‡æ–°åˆå¹¶æ•°æ®
        all_features = np.vstack([source_features, target_features, prototypes])

        print("æ‰§è¡Œ3D t-SNEé™ç»´...")
        tsne = TSNE(n_components=3, random_state=42, perplexity=30)
        features_3d = tsne.fit_transform(all_features)

        # åˆ†ç¦»ä¸åŒåŸŸçš„æ•°æ®
        n_source = len(source_features)
        n_target = len(target_features)

        source_3d = features_3d[:n_source]
        target_3d = features_3d[n_source:n_source+n_target]
        prototypes_3d = features_3d[n_source+n_target:]

        # ç»˜åˆ¶3Då¯¹é½å¯è§†åŒ–
        fig = plt.figure(figsize=(15, 10))
        ax = fig.add_subplot(111, projection='3d')

        # ç»˜åˆ¶æºåŸŸæ•°æ®ï¼ˆæŒ‰ç±»åˆ«ç€è‰²ï¼‰
        for i, (name, color) in enumerate(zip(self.fault_names, self.fault_colors)):
            mask = source_labels == i
            if np.any(mask):
                ax.scatter(source_3d[mask, 0], source_3d[mask, 1], source_3d[mask, 2],
                          c=color, alpha=0.6, s=20, label=f'Source {name}')

        # ç»˜åˆ¶ç›®æ ‡åŸŸæ•°æ®ï¼ˆç°è‰²ï¼‰
        ax.scatter(target_3d[:, 0], target_3d[:, 1], target_3d[:, 2],
                  c='gray', alpha=0.5, s=15, marker='^', label='Target Domain')

        # ç»˜åˆ¶åŸå‹ï¼ˆå¤§æ˜Ÿå·ï¼‰
        for i, (name, color) in enumerate(zip(self.fault_names, self.fault_colors)):
            ax.scatter(prototypes_3d[i, 0], prototypes_3d[i, 1], prototypes_3d[i, 2],
                      c=color, s=300, marker='*', edgecolor='black', linewidth=2,
                      label=f'{name} Prototype')

        ax.set_title('3DåŸŸå¯¹é½å¯è§†åŒ– (t-SNE) - æ‰€æœ‰é€šé“', fontsize=16, fontweight='bold')
        ax.set_xlabel('t-SNE Dimension 1')
        ax.set_ylabel('t-SNE Dimension 2')
        ax.set_zlabel('t-SNE Dimension 3')
        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.tight_layout()
        plt.savefig(os.path.join(self.plot_dir, 'domain_alignment_3d_all_channels.png'), dpi=300, bbox_inches='tight')
        plt.close()

    def _analyze_wasserstein_distances(self, source_features, target_features, prototypes):
        """åˆ†æWassersteinè·ç¦»"""
        print("åˆ†æåŸŸé—´Wassersteinè·ç¦»...")

        # è®¡ç®—ç›®æ ‡åŸŸç‰¹å¾åˆ°å„åŸå‹çš„è·ç¦»
        target_proto_distances = []
        for i, proto in enumerate(prototypes):
            distances = np.linalg.norm(target_features - proto, axis=1)
            target_proto_distances.append(distances)

        target_proto_distances = np.array(target_proto_distances).T  # (n_target, n_prototypes)

        # æ‰¾åˆ°æ¯ä¸ªç›®æ ‡åŸŸæ ·æœ¬æœ€è¿‘çš„åŸå‹
        closest_prototypes = np.argmin(target_proto_distances, axis=1)

        # ç»Ÿè®¡ç›®æ ‡åŸŸæ ·æœ¬åˆ†é…åˆ°å„åŸå‹çš„æ¯”ä¾‹
        assignment_counts = np.bincount(closest_prototypes, minlength=len(self.fault_names))
        assignment_ratios = assignment_counts / len(target_features)

        # ç»˜åˆ¶åˆ†é…æ¯”ä¾‹
        plt.figure(figsize=(10, 6))
        bars = plt.bar(self.fault_names, assignment_ratios, color=self.fault_colors, alpha=0.7)
        plt.title('ç›®æ ‡åŸŸæ ·æœ¬åˆ°åŸå‹çš„åˆ†é…æ¯”ä¾‹ - æ‰€æœ‰é€šé“', fontsize=14, fontweight='bold')
        plt.ylabel('åˆ†é…æ¯”ä¾‹')
        plt.xlabel('æ•…éšœåŸå‹')

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, ratio in zip(bars, assignment_ratios):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{ratio:.2%}', ha='center', va='bottom', fontweight='bold')

        plt.tight_layout()
        plt.savefig(os.path.join(self.plot_dir, 'target_assignment_all_channels.png'), dpi=300, bbox_inches='tight')
        plt.close()

        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        with open(os.path.join(self.report_dir, 'transfer_analysis_all_channels.txt'), 'w', encoding='utf-8') as f:
            f.write("è¿ç§»è¿‡ç¨‹åˆ†ææŠ¥å‘Š - æ‰€æœ‰é€šé“æ•°æ®\n")
            f.write("=" * 30 + "\n\n")
            f.write("æ•°æ®ç‰¹ç‚¹:\n")
            f.write("- åŸºäºDE + FE + BAä¸‰é€šé“èåˆè®­ç»ƒçš„PATE-Net\n")
            f.write("- æºåŸŸæ•°æ®é‡: çº¦49kæ ·æœ¬ï¼ˆç›¸æ¯”å•é€šé“3å€å¢é•¿ï¼‰\n")
            f.write("- å¤šé€šé“ä¿¡æ¯æå‡åŸŸå¯¹é½æ•ˆæœ\n\n")
            f.write("ç›®æ ‡åŸŸæ ·æœ¬åˆ†é…ç»Ÿè®¡:\n")
            for i, (name, count, ratio) in enumerate(zip(self.fault_names, assignment_counts, assignment_ratios)):
                f.write(f"{name}: {count} æ ·æœ¬ ({ratio:.2%})\n")

            f.write(f"\nå¹³å‡æœ€å°è·ç¦»: {np.mean(np.min(target_proto_distances, axis=1)):.4f}\n")

    def analyze_post_hoc_explainability(self):
        """äº‹åå¯è§£é‡Šæ€§ï¼šå†³ç­–è§£é‡Šå’Œæ¡ˆä¾‹åˆ†æ"""
        print("\n=== äº‹åå¯è§£é‡Šæ€§ï¼šå†³ç­–è§£é‡Šåˆ†æï¼ˆæ‰€æœ‰é€šé“ï¼‰ ===")

        # ç”Ÿæˆé¢„æµ‹è§£é‡Š
        self._generate_prediction_explanations()

        # é”™è¯¯æ¡ˆä¾‹åˆ†æ
        self._analyze_error_cases()

    def _generate_prediction_explanations(self):
        """ç”Ÿæˆé¢„æµ‹è§£é‡Šç¤ºä¾‹"""
        print("ç”Ÿæˆé¢„æµ‹è§£é‡Šç¤ºä¾‹...")

        # éšæœºé€‰æ‹©ä¸€äº›éªŒè¯æ ·æœ¬
        sample_indices = np.random.choice(len(self.val_dataset), 8, replace=False)

        explanations = []
        for idx in sample_indices:
            signal, true_label = self.val_dataset[idx]
            signal = signal.unsqueeze(0).to(self.device)

            with torch.no_grad():
                prediction, confidence, distances, explanation = self.model.predict_with_explanation(signal)

            explanations.append({
                'sample_idx': idx,
                'true_label': self.fault_names[true_label],
                'prediction': self.fault_names[prediction[0].item()],
                'confidence': confidence[0].item(),
                'distances': distances[0].cpu().numpy(),
                'explanation': explanation[0]
            })

        # ä¿å­˜è§£é‡Šç¤ºä¾‹
        with open(os.path.join(self.report_dir, 'prediction_explanations_all_channels.txt'), 'w', encoding='utf-8') as f:
            f.write("PATE-Net é¢„æµ‹è§£é‡Šç¤ºä¾‹ - æ‰€æœ‰é€šé“æ•°æ®\n")
            f.write("=" * 40 + "\n\n")
            f.write("æ¨¡å‹ç‰¹ç‚¹: åŸºäºDE + FE + BAä¸‰é€šé“èåˆè®­ç»ƒ\n")
            f.write("æ•°æ®ä¼˜åŠ¿: 49kæ ·æœ¬ï¼Œå¤šè§’åº¦ä¼ æ„Ÿå™¨ä¿¡æ¯äº’è¡¥\n\n")

            for i, exp in enumerate(explanations):
                f.write(f"ç¤ºä¾‹ {i+1}:\n")
                f.write(f"çœŸå®æ ‡ç­¾: {exp['true_label']}\n")
                f.write(f"é¢„æµ‹ç»“æœ: {exp['prediction']}\n")
                f.write(f"é¢„æµ‹ç½®ä¿¡åº¦: {exp['confidence']:.3f}\n")
                f.write("åˆ°å„åŸå‹çš„è·ç¦»:\n")
                for j, (name, dist) in enumerate(zip(self.fault_names, exp['distances'])):
                    f.write(f"  {name}: {dist:.3f}\n")
                f.write("\n" + "-" * 30 + "\n\n")

    def _analyze_error_cases(self):
        """åˆ†æé”™è¯¯æ¡ˆä¾‹"""
        print("åˆ†æé”™è¯¯é¢„æµ‹æ¡ˆä¾‹...")

        all_predictions = []
        all_labels = []
        all_distances = []

        with torch.no_grad():
            for signals, labels in self.val_loader:
                signals, labels = signals.to(self.device), labels.to(self.device)
                _, distances, logits = self.model(signals)

                predictions = torch.argmax(logits, dim=1)
                all_predictions.extend(predictions.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                all_distances.extend(distances.cpu().numpy())

        all_predictions = np.array(all_predictions)
        all_labels = np.array(all_labels)
        all_distances = np.array(all_distances)

        # æ‰¾å‡ºé”™è¯¯é¢„æµ‹
        error_mask = all_predictions != all_labels
        error_indices = np.where(error_mask)[0]

        if len(error_indices) > 0:
            # åˆ†æé”™è¯¯ç±»å‹
            error_analysis = {}
            for true_label in range(self.args.num_classes):
                for pred_label in range(self.args.num_classes):
                    if true_label != pred_label:
                        mask = (all_labels == true_label) & (all_predictions == pred_label)
                        count = np.sum(mask)
                        if count > 0:
                            error_analysis[f"{self.fault_names[true_label]} -> {self.fault_names[pred_label]}"] = count

            # ä¿å­˜é”™è¯¯åˆ†æ
            with open(os.path.join(self.report_dir, 'error_analysis_all_channels.txt'), 'w', encoding='utf-8') as f:
                f.write("é”™è¯¯é¢„æµ‹åˆ†æ - æ‰€æœ‰é€šé“æ•°æ®\n")
                f.write("=" * 20 + "\n\n")
                f.write("æ¨¡å‹ç‰¹ç‚¹: åŸºäºDE + FE + BAä¸‰é€šé“èåˆçš„PATE-Net\n")
                f.write(f"æ•°æ®è§„æ¨¡: çº¦49kæ ·æœ¬è®­ç»ƒï¼Œå¤šé€šé“ä¿¡æ¯èåˆ\n\n")
                f.write(f"æ€»é”™è¯¯æ•°: {len(error_indices)}\n")
                f.write(f"æ€»å‡†ç¡®ç‡: {(len(all_labels) - len(error_indices)) / len(all_labels):.4f}\n\n")
                f.write("ä¸»è¦é”™è¯¯ç±»å‹:\n")
                for error_type, count in sorted(error_analysis.items(), key=lambda x: x[1], reverse=True):
                    f.write(f"- {error_type}: {count} æ¬¡\n")

    def generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆå¯è§£é‡Šæ€§æŠ¥å‘Š"""
        print("\n=== ç”Ÿæˆç»¼åˆå¯è§£é‡Šæ€§æŠ¥å‘Šï¼ˆæ‰€æœ‰é€šé“ï¼‰ ===")

        from datetime import datetime

        with open(os.path.join(self.report_dir, 'comprehensive_explainability_report_all_channels.md'), 'w', encoding='utf-8') as f:
            f.write("# PATE-Net å¯è§£é‡Šæ€§ç»¼åˆæŠ¥å‘Š - æ‰€æœ‰é€šé“æ•°æ®\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

            f.write("## æ•°æ®å¢å¼ºç­–ç•¥\n\n")
            f.write("æœ¬æŠ¥å‘ŠåŸºäºæ‰€æœ‰é€šé“æ•°æ®ï¼ˆDE + FE + BAï¼‰è®­ç»ƒçš„PATE-Netæ¨¡å‹ï¼Œå®ç°äº†ï¼š\n\n")
            f.write("- **æ•°æ®æ‰©å¢**: ä»16kå¢é•¿åˆ°49kæ ·æœ¬ï¼ˆ3å€å¢é•¿ï¼‰\n")
            f.write("- **ä¿¡æ¯èåˆ**: ä¸‰é€šé“ä¼ æ„Ÿå™¨ä¿¡æ¯äº’è¡¥\n")
            f.write("- **é²æ£’æå‡**: å¤šæºä¿¡æ¯æå‡è¯Šæ–­å¯é æ€§\n\n")

            f.write("## æ–¹æ¡ˆæ¦‚è¿°\n\n")
            f.write("PATE-Net (Prototypical Alignment for Transferable Explainable Network) æ˜¯åŸºäºåŸå‹å¯¹é½çš„å¯è§£é‡Šè¿ç§»è¯Šæ–­ç½‘ç»œï¼Œ")
            f.write("åœ¨æ‰€æœ‰é€šé“æ•°æ®åŸºç¡€ä¸Šå®ç°äº†è½´æ‰¿æ•…éšœè¯Šæ–­çš„å…¨æ–¹ä½å¯è§£é‡Šæ€§ã€‚\n\n")

            f.write("## æ ¸å¿ƒåˆ›æ–°ï¼ˆæ‰€æœ‰é€šé“ç‰ˆæœ¬ï¼‰\n\n")
            f.write("### 1. å¤šé€šé“åŸå‹ç›‘ç£å¯¹æ¯”å­¦ä¹ \n")
            f.write("- èåˆDEã€FEã€BAä¸‰é€šé“ä¿¡æ¯å­¦ä¹ æ•…éšœåŸå‹\n")
            f.write("- åŸºäº49kæ ·æœ¬çš„å¤§è§„æ¨¡è®­ç»ƒ\n")
            f.write("- å¤šè§’åº¦ä¼ æ„Ÿå™¨ä¿¡æ¯å¢å¼ºåŸå‹è´¨é‡\n")
            f.write("- å®ç°å¤©ç„¶çš„äº‹å‰å¯è§£é‡Šæ€§\n\n")

            f.write("### 2. å¢å¼ºçš„æœ€ä¼˜ä¼ è¾“åŸŸå¯¹é½\n")
            f.write("- åŸºäºå¤šé€šé“ç‰¹å¾çš„Wassersteinè·ç¦»\n")
            f.write("- 3Då¯è§†åŒ–ç›®æ ‡åŸŸæ•°æ®å‘æºåŸŸåŸå‹çš„'æµåŠ¨'è¿‡ç¨‹\n")
            f.write("- å¤šé€šé“ä¿¡æ¯æå‡åŸŸå¯¹é½æ•ˆæœ\n")
            f.write("- å®ç°è¿ç§»è¿‡ç¨‹çš„å¯è§£é‡Šæ€§\n\n")

            f.write("### 3. é²æ£’çš„è·ç¦»åŸºå†³ç­–æœºåˆ¶\n")
            f.write("- åŸºäºå¤šé€šé“ç‰¹å¾ä¸åŸå‹çš„è·ç¦»å†³ç­–\n")
            f.write("- æ¯ä¸ªé¢„æµ‹éƒ½æœ‰æ˜ç¡®çš„æ•°å€¼ä¾æ®\n")
            f.write("- å•é€šé“æ•…éšœä¸å½±å“æ•´ä½“è¯Šæ–­\n")
            f.write("- å®ç°äº‹åå¯è§£é‡Šæ€§\n\n")

            f.write("## å¯è§£é‡Šæ€§ç»´åº¦\n\n")
            f.write("### äº‹å‰å¯è§£é‡Šæ€§ âœ…\n")
            f.write("- å¤šé€šé“åŸå‹ç½‘ç»œæ¶æ„é€æ˜\n")
            f.write("- å†³ç­–æœºåˆ¶ç®€å•æ˜äº†\n")
            f.write("- åŸºäº49kæ ·æœ¬çš„å……åˆ†è®­ç»ƒ\n")
            f.write("- æ— éœ€é¢å¤–è§£é‡Šå·¥å…·\n\n")

            f.write("### è¿ç§»è¿‡ç¨‹å¯è§£é‡Šæ€§ âœ…\n")
            f.write("- å¤šé€šé“Wassersteinè·ç¦»é‡åŒ–åŸŸå·®å¼‚\n")
            f.write("- 3D t-SNEå¯è§†åŒ–å¯¹é½è¿‡ç¨‹\n")
            f.write("- ç›®æ ‡åŸŸæ ·æœ¬åˆ†é…ç»Ÿè®¡\n")
            f.write("- å¤šæºä¿¡æ¯èåˆæ•ˆæœå±•ç¤º\n\n")

            f.write("### äº‹åå¯è§£é‡Šæ€§ âœ…\n")
            f.write("- åŸºäºå¤šé€šé“çš„è·ç¦»åŸºé¢„æµ‹è§£é‡Š\n")
            f.write("- é”™è¯¯æ¡ˆä¾‹è¯¦ç»†åˆ†æ\n")
            f.write("- é¢„æµ‹ç½®ä¿¡åº¦é‡åŒ–\n")
            f.write("- é²æ£’æ€§ä¼˜åŠ¿ä½“ç°\n\n")

            f.write("## æ‰€æœ‰é€šé“ä¼˜åŠ¿å¯¹æ¯”\n\n")
            f.write("| ç‰¹æ€§ | å•é€šé“(DE only) | æ‰€æœ‰é€šé“(DE+FE+BA) |\n")
            f.write("|------|----------------|-------------------|\n")
            f.write("| æ ·æœ¬æ•°é‡ | 16k | 49k (3å€å¢é•¿) |\n")
            f.write("| ä¿¡æ¯æº | å•ä¸€é©±åŠ¨ç«¯ | ä¸‰é€šé“èåˆ |\n")
            f.write("| é²æ£’æ€§ | ä¸€èˆ¬ | æ˜¾è‘—æå‡ |\n")
            f.write("| æ•…éšœè¦†ç›– | æœ‰é™ | å…¨é¢ |\n")
            f.write("| è¯Šæ–­å¯é æ€§ | ä¸­ç­‰ | é«˜ |\n\n")

            f.write("## ç”Ÿæˆæ–‡ä»¶\n\n")
            f.write("- `plots/prototypes_visualization_3d_all_channels.png`: 3DåŸå‹å¯è§†åŒ–\n")
            f.write("- `plots/prototype_distances_all_channels.png`: åŸå‹é—´è·ç¦»\n")
            f.write("- `plots/domain_alignment_3d_all_channels.png`: 3DåŸŸå¯¹é½å¯è§†åŒ–\n")
            f.write("- `plots/target_assignment_all_channels.png`: ç›®æ ‡åŸŸåˆ†é…\n")
            f.write("- `reports/architecture_transparency_all_channels.md`: æ¶æ„é€æ˜æ€§\n")
            f.write("- `reports/transfer_analysis_all_channels.txt`: è¿ç§»è¿‡ç¨‹åˆ†æ\n")
            f.write("- `reports/prediction_explanations_all_channels.txt`: é¢„æµ‹è§£é‡Š\n")
            f.write("- `reports/error_analysis_all_channels.txt`: é”™è¯¯åˆ†æ\n\n")

        print("âœ“ ç»¼åˆå¯è§£é‡Šæ€§æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼ˆæ‰€æœ‰é€šé“ç‰ˆæœ¬ï¼‰")

    def run_analysis(self):
        """è¿è¡Œå®Œæ•´çš„å¯è§£é‡Šæ€§åˆ†æ"""
        print("å¼€å§‹PATE-Netå¯è§£é‡Šæ€§åˆ†æï¼ˆæ‰€æœ‰é€šé“æ•°æ®ï¼‰...")

        # åŠ è½½æ¨¡å‹å’Œæ•°æ®
        if not self.load_model_and_data():
            return False

        # æ‰§è¡Œä¸‰ç»´åº¦å¯è§£é‡Šæ€§åˆ†æ
        self.analyze_ex_ante_interpretability()    # äº‹å‰å¯è§£é‡Šæ€§
        self.analyze_transfer_process()            # è¿ç§»è¿‡ç¨‹å¯è§£é‡Šæ€§
        self.analyze_post_hoc_explainability()     # äº‹åå¯è§£é‡Šæ€§
        self.generate_comprehensive_report()      # ç»¼åˆæŠ¥å‘Š

        print(f"\nğŸ‰ PATE-Netå¯è§£é‡Šæ€§åˆ†æå®Œæˆï¼ˆæ‰€æœ‰é€šé“ï¼‰ï¼")
        print(f"ğŸ“Š æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {self.args.output_dir}")
        return True

def main():
    parser = argparse.ArgumentParser(description='PATE-Netå¯è§£é‡Šæ€§åˆ†æ - æ‰€æœ‰é€šé“æ•°æ®')

    # æ•°æ®è·¯å¾„
    parser.add_argument('--source_data_path', type=str,
                        default='processed_data_all_channels/source_train_all_channels.npz',
                        help='æºåŸŸæ•°æ®è·¯å¾„ï¼ˆæ‰€æœ‰é€šé“ï¼‰')
    parser.add_argument('--target_data_path', type=str,
                        default='processed_data_all_channels/source_val_all_channels.npz',
                        help='ç›®æ ‡åŸŸæ•°æ®è·¯å¾„ï¼ˆæ‰€æœ‰é€šé“ï¼‰')
    parser.add_argument('--val_data_path', type=str,
                        default='processed_data_all_channels/source_val_all_channels.npz',
                        help='éªŒè¯æ•°æ®è·¯å¾„ï¼ˆæ‰€æœ‰é€šé“ï¼‰')

    # æ¨¡å‹è·¯å¾„
    parser.add_argument('--model_path', type=str,
                        default='models_saved/pate_net_all_channels/best_pate_aligned_model.pth',
                        help='PATE-Netæ¨¡å‹è·¯å¾„ï¼ˆæ‰€æœ‰é€šé“ï¼‰')

    # æ¨¡å‹å‚æ•°
    parser.add_argument('--signal_length', type=int, default=2048, help='ä¿¡å·é•¿åº¦')
    parser.add_argument('--feature_dim', type=int, default=256, help='ç‰¹å¾ç»´åº¦')
    parser.add_argument('--num_classes', type=int, default=4, help='åˆ†ç±»ç±»åˆ«æ•°')
    parser.add_argument('--temperature', type=float, default=0.05, help='æ¸©åº¦å‚æ•°')

    # è¾“å‡ºè·¯å¾„
    parser.add_argument('--output_dir', type=str, default='results/pate_explainability_all_channels',
                        help='å¯è§£é‡Šæ€§åˆ†æç»“æœè¾“å‡ºç›®å½•ï¼ˆæ‰€æœ‰é€šé“ï¼‰')

    args = parser.parse_args()

    # åˆ›å»ºåˆ†æå™¨å¹¶è¿è¡Œ
    explainer = PATE_ExplainerAllChannels(args)
    explainer.run_analysis()

if __name__ == '__main__':
    main()