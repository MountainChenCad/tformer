"""
æ”¹è¿›çš„å¤šç»´åº¦å¯è§£é‡Šæ€§åˆ†æè„šæœ¬ - åŸºäºæ‰€æœ‰é€šé“æ•°æ®
åŸºäºè½´æ‰¿æ•…éšœè¯Šæ–­çš„SCTL-FDæ¡†æ¶ï¼Œå®ç°ï¼š
1. äº‹å‰å¯è§£é‡Šæ€§ï¼šæ¨¡å‹æ¶æ„é€æ˜æ€§åˆ†æ
2. è¿ç§»è¿‡ç¨‹å¯è§£é‡Šæ€§ï¼šç‰¹å¾è¿ç§»è·¯å¾„å’Œæ¨¡å¼åˆ†æ
3. äº‹åå¯è§£é‡Šæ€§ï¼šå†³ç­–ä¾æ®å’Œç‰¹å¾é‡è¦æ€§åˆ†æ
"""

import os
import sys
import argparse
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®è‹±æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial']
plt.rcParams['axes.unicode_minus'] = False

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from src.models import create_lstm_classification_model, create_lstm_supcon_model, load_pretrained_encoder
from src.datasets import StandardDataset, SupConDataset

class AdvancedExplainabilityAnalyzerAllChannels:
    """åŸºäºæ‰€æœ‰é€šé“æ•°æ®çš„é«˜çº§å¯è§£é‡Šæ€§åˆ†æå™¨"""

    def __init__(self, args):
        self.args = args
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")

        # æ•…éšœç±»å‹æ˜ å°„ (4-class problem: Normal, Inner, Outer, Ball)
        self.fault_names = ['Normal', 'Inner', 'Outer', 'Ball']
        self.fault_colors = ['blue', 'red', 'green', 'orange']

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(args.output_dir, exist_ok=True)

        # åˆ›å»ºå­ç›®å½•
        self.plot_dir = os.path.join(args.output_dir, 'plots')
        self.report_dir = os.path.join(args.output_dir, 'reports')
        os.makedirs(self.plot_dir, exist_ok=True)
        os.makedirs(self.report_dir, exist_ok=True)

    def load_models(self):
        """åŠ è½½é¢„è®­ç»ƒå’Œå¾®è°ƒåçš„æ¨¡å‹"""
        print("åŠ è½½åŸºäºæ‰€æœ‰é€šé“è®­ç»ƒçš„æ¨¡å‹...")

        # 1. åŠ è½½é¢„è®­ç»ƒSupConæ¨¡å‹ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
        if os.path.exists(self.args.pretrained_encoder_path):
            self.pretrained_encoder = create_lstm_supcon_model(
                signal_length=self.args.signal_length,
                feature_dim=self.args.feature_dim,
                hidden_dim=self.args.hidden_dim,
                num_layers=self.args.num_layers
            ).encoder
            self.pretrained_encoder.load_state_dict(torch.load(self.args.pretrained_encoder_path, map_location=self.device))
            self.pretrained_encoder.to(self.device)
            self.pretrained_encoder.eval()
            print("âœ“ é¢„è®­ç»ƒencoderåŠ è½½æˆåŠŸ")
        else:
            print("âš  é¢„è®­ç»ƒencoderä¸å­˜åœ¨ï¼Œè·³è¿‡å¯¹æ¯”åˆ†æ")
            self.pretrained_encoder = None

        # 2. åŠ è½½å¾®è°ƒåçš„å®Œæ•´æ¨¡å‹
        if os.path.exists(self.args.final_model_path):
            self.final_model = create_lstm_classification_model(
                signal_length=self.args.signal_length,
                feature_dim=self.args.feature_dim,
                hidden_dim=self.args.hidden_dim,
                num_layers=self.args.num_layers,
                num_classes=self.args.num_classes
            )
            self.final_model.load_state_dict(torch.load(self.args.final_model_path, map_location=self.device))
            self.final_model.to(self.device)
            self.final_model.eval()
            print("âœ“ æœ€ç»ˆæ¨¡å‹åŠ è½½æˆåŠŸ")
        else:
            print("âœ— æœ€ç»ˆæ¨¡å‹ä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
            return False

        return True

    def load_data(self):
        """åŠ è½½æ•°æ®"""
        print("åŠ è½½æ‰€æœ‰é€šé“æ•°æ®...")

        # åŠ è½½éªŒè¯æ•°æ®
        self.val_dataset = StandardDataset(self.args.val_data_path)
        self.val_loader = DataLoader(
            self.val_dataset,
            batch_size=self.args.batch_size,
            shuffle=False,
            num_workers=4
        )

        print(f"éªŒè¯æ•°æ®: {len(self.val_dataset)} æ ·æœ¬")
        print(f"æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(self.val_dataset.labels)}")

        return True

    def extract_features(self, model, data_loader, use_encoder_only=False):
        """æå–æ¨¡å‹ç‰¹å¾"""
        features = []
        labels = []
        predictions = []

        with torch.no_grad():
            for data, label in data_loader:
                data, label = data.to(self.device), label.to(self.device)

                if use_encoder_only:
                    # åªä½¿ç”¨encoderæå–ç‰¹å¾
                    feature = model(data)
                else:
                    # ä½¿ç”¨å®Œæ•´æ¨¡å‹
                    if hasattr(model, 'encoder'):
                        feature = model.encoder(data)
                        logits = model.classifier_head(feature)
                    else:
                        feature = model.features(data)
                        logits = model.classifier_head(feature)

                    pred = torch.argmax(logits, dim=1)
                    predictions.extend(pred.cpu().numpy())

                features.extend(feature.cpu().numpy())
                labels.extend(label.cpu().numpy())

        return np.array(features), np.array(labels), np.array(predictions) if predictions else None

    def analyze_model_architecture(self):
        """äº‹å‰å¯è§£é‡Šæ€§ï¼šæ¨¡å‹æ¶æ„åˆ†æ"""
        print("\n=== äº‹å‰å¯è§£é‡Šæ€§ï¼šæ¨¡å‹æ¶æ„åˆ†æï¼ˆæ‰€æœ‰é€šé“æ•°æ®ï¼‰ ===")

        report = []
        report.append("# SCTL-FD æ¨¡å‹æ¶æ„é€æ˜æ€§åˆ†ææŠ¥å‘Š - æ‰€æœ‰é€šé“æ•°æ®\n")

        # åˆ†ææ¨¡å‹ç»“æ„
        if self.final_model:
            total_params = sum(p.numel() for p in self.final_model.parameters())
            trainable_params = sum(p.numel() for p in self.final_model.parameters() if p.requires_grad)

            report.append(f"## æ¨¡å‹ç»“æ„æ¦‚è§ˆ\n")
            report.append(f"- **æ•°æ®æº**: æ‰€æœ‰é€šé“æ•°æ® (DE + FE + BA)")
            report.append(f"- **æ•°æ®å¢å¼º**: 3å€æ‰©å¢ (çº¦49kæ ·æœ¬)")
            report.append(f"- **æ€»å‚æ•°æ•°**: {total_params:,}")
            report.append(f"- **å¯è®­ç»ƒå‚æ•°æ•°**: {trainable_params:,}")
            report.append(f"- **æ¨¡å‹æ·±åº¦**: LSTM {self.args.num_layers} å±‚")
            report.append(f"- **éšè—ç»´åº¦**: {self.args.hidden_dim}")
            report.append(f"- **ç‰¹å¾ç»´åº¦**: {self.args.feature_dim}")
            report.append(f"- **åˆ†ç±»ç±»åˆ«**: {self.args.num_classes} (Normal, Inner, Outer, Ball)\n")

            # æ•°æ®æ‰©å¢ä¼˜åŠ¿åˆ†æ
            report.append(f"## æ‰€æœ‰é€šé“æ•°æ®ä¼˜åŠ¿\n")
            report.append(f"- **DEé€šé“**: é©±åŠ¨ç«¯ä¿¡å·ï¼Œæ•…éšœç‰¹å¾æœ€ç›´æ¥")
            report.append(f"- **FEé€šé“**: é£æ‰‡ç«¯ä¿¡å·ï¼Œæä¾›ä¸åŒè§†è§’")
            report.append(f"- **BAé€šé“**: åŸºåº§ä¿¡å·ï¼Œæä¾›æ•´ä½“æŒ¯åŠ¨ç‰¹å¾")
            report.append(f"- **æ•°æ®èåˆ**: å¤šé€šé“ä¿¡æ¯èåˆæå‡è¯Šæ–­é²æ£’æ€§")
            report.append(f"- **æ ·æœ¬å¢å¼º**: ä»16kæ‰©å¢åˆ°49kï¼Œç¼“è§£æ•°æ®ç¨€ç¼ºé—®é¢˜\n")

            # åˆ†æå„å±‚å‚æ•°åˆ†å¸ƒ
            report.append(f"## å„å±‚å‚æ•°åˆ†æ\n")
            for name, module in self.final_model.named_modules():
                if len(list(module.parameters())) > 0:
                    layer_params = sum(p.numel() for p in module.parameters())
                    report.append(f"- **{name}**: {layer_params:,} å‚æ•°")

            report.append(f"\n## æ¨¡å‹è®¾è®¡ç†å¿µ\n")
            report.append(f"- **LSTMæ¶æ„**: é€‚åˆæ—¶åºä¿¡å·å»ºæ¨¡ï¼Œèƒ½å¤Ÿæ•è·æŒ¯åŠ¨ä¿¡å·çš„æ—¶é—´ä¾èµ–æ€§")
            report.append(f"- **ç›‘ç£å¯¹æ¯”å­¦ä¹ **: é¢„è®­ç»ƒé˜¶æ®µå­¦ä¹ åˆ¤åˆ«æ€§ç‰¹å¾è¡¨ç¤º")
            report.append(f"- **è¿ç§»å­¦ä¹ **: å†»ç»“é¢„è®­ç»ƒç‰¹å¾ï¼Œåªå¾®è°ƒåˆ†ç±»å™¨")
            report.append(f"- **å¤šé€šé“èåˆ**: æ•´åˆä¸åŒä¼ æ„Ÿå™¨ä½ç½®çš„äº’è¡¥ä¿¡æ¯")
            report.append(f"- **4åˆ†ç±»è®¾è®¡**: åŒ…å«æ­£å¸¸çŠ¶æ€ï¼Œç¬¦åˆå·¥ä¸šåº”ç”¨éœ€æ±‚\n")

        # ä¿å­˜æ¶æ„åˆ†ææŠ¥å‘Š
        with open(os.path.join(self.report_dir, 'architecture_analysis_all_channels.md'), 'w', encoding='utf-8') as f:
            f.write('\n'.join(report))

        print("âœ“ æ¨¡å‹æ¶æ„åˆ†æå®Œæˆ")

    def analyze_feature_transfer(self):
        """è¿ç§»è¿‡ç¨‹å¯è§£é‡Šæ€§ï¼šç‰¹å¾è¿ç§»åˆ†æ"""
        print("\n=== è¿ç§»è¿‡ç¨‹å¯è§£é‡Šæ€§ï¼šç‰¹å¾è¿ç§»åˆ†æï¼ˆæ‰€æœ‰é€šé“ï¼‰ ===")

        if self.pretrained_encoder is None:
            print("âš  ç¼ºå°‘é¢„è®­ç»ƒencoderï¼Œè·³è¿‡è¿ç§»åˆ†æ")
            return

        # æå–é¢„è®­ç»ƒå’Œå¾®è°ƒåçš„ç‰¹å¾
        print("æå–é¢„è®­ç»ƒç‰¹å¾ï¼ˆæ‰€æœ‰é€šé“ï¼‰...")
        pretrained_features, labels, _ = self.extract_features(
            self.pretrained_encoder, self.val_loader, use_encoder_only=True
        )

        print("æå–å¾®è°ƒåç‰¹å¾ï¼ˆæ‰€æœ‰é€šé“ï¼‰...")
        final_features, _, predictions = self.extract_features(
            self.final_model, self.val_loader, use_encoder_only=False
        )

        # ä½¿ç”¨3D t-SNEé™ç»´å¯è§†åŒ–ï¼ˆä¸å•é€šé“ç‰ˆæœ¬ä¿æŒä¸€è‡´ï¼‰
        print("è¿›è¡Œ3D t-SNEé™ç»´...")

        # é‡‡æ ·æ•°æ®ä»¥åŠ é€Ÿè®¡ç®—
        sample_size = min(1000, len(pretrained_features))
        indices = np.random.choice(len(pretrained_features), sample_size, replace=False)

        pretrained_sample = pretrained_features[indices]
        final_sample = final_features[indices]
        labels_sample = labels[indices]

        # 3D t-SNEé™ç»´
        tsne = TSNE(n_components=3, random_state=42, perplexity=30)
        pretrained_tsne = tsne.fit_transform(pretrained_sample)

        tsne = TSNE(n_components=3, random_state=42, perplexity=30)
        final_tsne = tsne.fit_transform(final_sample)

        # ç»˜åˆ¶3Då¯¹æ¯”å›¾
        fig = plt.figure(figsize=(20, 8))

        # é¢„è®­ç»ƒç‰¹å¾åˆ†å¸ƒï¼ˆ3Dï¼‰
        ax1 = fig.add_subplot(121, projection='3d')
        for i, (name, color) in enumerate(zip(self.fault_names, self.fault_colors)):
            mask = labels_sample == i
            ax1.scatter(pretrained_tsne[mask, 0], pretrained_tsne[mask, 1], pretrained_tsne[mask, 2],
                       c=color, label=name, alpha=0.6, s=20)
        ax1.set_title('é¢„è®­ç»ƒç‰¹å¾åˆ†å¸ƒ (3D t-SNE) - æ‰€æœ‰é€šé“', fontsize=14, fontweight='bold')
        ax1.set_xlabel('t-SNE Dimension 1')
        ax1.set_ylabel('t-SNE Dimension 2')
        ax1.set_zlabel('t-SNE Dimension 3')
        ax1.legend()

        # å¾®è°ƒåç‰¹å¾åˆ†å¸ƒï¼ˆ3Dï¼‰
        ax2 = fig.add_subplot(122, projection='3d')
        for i, (name, color) in enumerate(zip(self.fault_names, self.fault_colors)):
            mask = labels_sample == i
            ax2.scatter(final_tsne[mask, 0], final_tsne[mask, 1], final_tsne[mask, 2],
                       c=color, label=name, alpha=0.6, s=20)
        ax2.set_title('å¾®è°ƒåç‰¹å¾åˆ†å¸ƒ (3D t-SNE) - æ‰€æœ‰é€šé“', fontsize=14, fontweight='bold')
        ax2.set_xlabel('t-SNE Dimension 1')
        ax2.set_ylabel('t-SNE Dimension 2')
        ax2.set_zlabel('t-SNE Dimension 3')
        ax2.legend()

        plt.tight_layout()
        plt.savefig(os.path.join(self.plot_dir, 'feature_transfer_analysis_all_channels_3d.png'),
                   dpi=300, bbox_inches='tight')
        plt.close()

        # è®¡ç®—ç‰¹å¾ç©ºé—´å˜åŒ–æŒ‡æ ‡
        print("è®¡ç®—ç‰¹å¾ç©ºé—´å˜åŒ–æŒ‡æ ‡...")

        # ç±»é—´è·ç¦»åˆ†æ
        class_centers_pre = []
        class_centers_final = []

        for i in range(self.args.num_classes):
            mask = labels_sample == i
            if np.sum(mask) > 0:
                center_pre = np.mean(pretrained_sample[mask], axis=0)
                center_final = np.mean(final_sample[mask], axis=0)
                class_centers_pre.append(center_pre)
                class_centers_final.append(center_final)

        class_centers_pre = np.array(class_centers_pre)
        class_centers_final = np.array(class_centers_final)

        # è®¡ç®—ç±»é—´è·ç¦»çŸ©é˜µ
        def compute_inter_class_distances(centers):
            n_classes = len(centers)
            distances = np.zeros((n_classes, n_classes))
            for i in range(n_classes):
                for j in range(n_classes):
                    distances[i, j] = np.linalg.norm(centers[i] - centers[j])
            return distances

        distances_pre = compute_inter_class_distances(class_centers_pre)
        distances_final = compute_inter_class_distances(class_centers_final)

        # ç»˜åˆ¶ç±»é—´è·ç¦»çƒ­åŠ›å›¾
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

        sns.heatmap(distances_pre, annot=True, fmt='.2f', cmap='Blues',
                   xticklabels=self.fault_names, yticklabels=self.fault_names, ax=ax1)
        ax1.set_title('é¢„è®­ç»ƒæ¨¡å‹ç±»é—´è·ç¦» - æ‰€æœ‰é€šé“')

        sns.heatmap(distances_final, annot=True, fmt='.2f', cmap='Oranges',
                   xticklabels=self.fault_names, yticklabels=self.fault_names, ax=ax2)
        ax2.set_title('å¾®è°ƒåæ¨¡å‹ç±»é—´è·ç¦» - æ‰€æœ‰é€šé“')

        plt.tight_layout()
        plt.savefig(os.path.join(self.plot_dir, 'inter_class_distances_all_channels.png'),
                   dpi=300, bbox_inches='tight')
        plt.close()

        print("âœ“ ç‰¹å¾è¿ç§»åˆ†æå®Œæˆ")

    def analyze_decision_basis(self):
        """äº‹åå¯è§£é‡Šæ€§ï¼šå†³ç­–ä¾æ®åˆ†æ"""
        print("\n=== äº‹åå¯è§£é‡Šæ€§ï¼šå†³ç­–ä¾æ®åˆ†æï¼ˆæ‰€æœ‰é€šé“ï¼‰ ===")

        # æå–ç‰¹å¾å’Œé¢„æµ‹
        features, labels, predictions = self.extract_features(
            self.final_model, self.val_loader, use_encoder_only=False
        )

        # 1. åˆ†ç±»æ€§èƒ½åˆ†æ
        print("åˆ†æåˆ†ç±»æ€§èƒ½...")

        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(labels, predictions)

        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=self.fault_names, yticklabels=self.fault_names)
        plt.title('æ··æ·†çŸ©é˜µ - æ‰€æœ‰é€šé“æ•°æ®', fontsize=16, fontweight='bold')
        plt.xlabel('é¢„æµ‹æ ‡ç­¾')
        plt.ylabel('çœŸå®æ ‡ç­¾')
        plt.tight_layout()
        plt.savefig(os.path.join(self.plot_dir, 'confusion_matrix_all_channels.png'),
                   dpi=300, bbox_inches='tight')
        plt.close()

        # åˆ†ç±»æŠ¥å‘Š
        report = classification_report(labels, predictions,
                                     target_names=self.fault_names,
                                     output_dict=True)

        # ä¿å­˜è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
        report_text = classification_report(labels, predictions,
                                          target_names=self.fault_names)

        with open(os.path.join(self.report_dir, 'classification_report_all_channels.txt'), 'w') as f:
            f.write("SCTL-FD 4åˆ†ç±»æ€§èƒ½æŠ¥å‘Š - æ‰€æœ‰é€šé“æ•°æ®\n")
            f.write("=" * 50 + "\n\n")
            f.write("æ•°æ®ç‰¹ç‚¹:\n")
            f.write("- æ•°æ®æº: DE + FE + BA ä¸‰é€šé“èåˆ\n")
            f.write("- æ ·æœ¬æ•°é‡: çº¦49k (ç›¸æ¯”å•é€šé“3å€å¢é•¿)\n")
            f.write("- æ•°æ®è´¨é‡: å¤šè§’åº¦ä¼ æ„Ÿå™¨ä¿¡æ¯äº’è¡¥\n\n")
            f.write(report_text)
            f.write("\n\nè¯¦ç»†åˆ†æ:\n")
            f.write(f"æ€»ä½“å‡†ç¡®ç‡: {report['accuracy']:.4f}\n")
            f.write(f"å®å¹³å‡F1: {report['macro avg']['f1-score']:.4f}\n")
            f.write(f"åŠ æƒå¹³å‡F1: {report['weighted avg']['f1-score']:.4f}\n")

        # 2. ç‰¹å¾é‡è¦æ€§åˆ†æï¼ˆä½¿ç”¨éšæœºæ£®æ—ï¼‰
        print("åˆ†æç‰¹å¾é‡è¦æ€§...")

        # è®­ç»ƒéšæœºæ£®æ—åˆ†ç±»å™¨
        rf = RandomForestClassifier(n_estimators=100, random_state=42)
        rf.fit(features, labels)

        # è·å–ç‰¹å¾é‡è¦æ€§
        feature_importance = rf.feature_importances_

        # ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§
        plt.figure(figsize=(12, 6))
        indices = np.argsort(feature_importance)[::-1][:50]  # æ˜¾ç¤ºå‰50ä¸ªé‡è¦ç‰¹å¾

        plt.bar(range(len(indices)), feature_importance[indices])
        plt.title('ç‰¹å¾é‡è¦æ€§æ’åº (Top 50) - æ‰€æœ‰é€šé“æ•°æ®', fontsize=14, fontweight='bold')
        plt.xlabel('ç‰¹å¾ç´¢å¼•')
        plt.ylabel('é‡è¦æ€§å¾—åˆ†')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(os.path.join(self.plot_dir, 'feature_importance_all_channels.png'),
                   dpi=300, bbox_inches='tight')
        plt.close()

        # 3. é”™è¯¯æ¡ˆä¾‹åˆ†æ
        print("åˆ†æé”™è¯¯æ¡ˆä¾‹...")

        # æ‰¾å‡ºé”™è¯¯åˆ†ç±»çš„æ ·æœ¬
        error_mask = predictions != labels
        error_indices = np.where(error_mask)[0]

        if len(error_indices) > 0:
            # åˆ†æé”™è¯¯ç±»å‹
            error_analysis = {}
            for true_label in range(self.args.num_classes):
                for pred_label in range(self.args.num_classes):
                    if true_label != pred_label:
                        mask = (labels == true_label) & (predictions == pred_label)
                        count = np.sum(mask)
                        if count > 0:
                            error_analysis[f"{self.fault_names[true_label]} -> {self.fault_names[pred_label]}"] = count

            # ä¿å­˜é”™è¯¯åˆ†æ
            with open(os.path.join(self.report_dir, 'error_analysis_all_channels.txt'), 'w', encoding='utf-8') as f:
                f.write("é”™è¯¯åˆ†ç±»åˆ†æ - æ‰€æœ‰é€šé“æ•°æ®\n")
                f.write("=" * 30 + "\n\n")
                f.write(f"æ€»é”™è¯¯æ•°: {len(error_indices)}\n")
                f.write(f"é”™è¯¯ç‡: {len(error_indices)/len(labels)*100:.2f}%\n\n")
                f.write("ä¸»è¦é”™è¯¯ç±»å‹:\n")
                for error_type, count in sorted(error_analysis.items(), key=lambda x: x[1], reverse=True):
                    f.write(f"- {error_type}: {count} æ¬¡\n")

        print("âœ“ å†³ç­–ä¾æ®åˆ†æå®Œæˆ")

    def generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆå¯è§£é‡Šæ€§æŠ¥å‘Š"""
        print("\n=== ç”Ÿæˆç»¼åˆå¯è§£é‡Šæ€§æŠ¥å‘Šï¼ˆæ‰€æœ‰é€šé“ï¼‰ ===")

        report = []
        report.append("# SCTL-FD è½´æ‰¿æ•…éšœè¯Šæ–­å¯è§£é‡Šæ€§ç»¼åˆæŠ¥å‘Š - æ‰€æœ‰é€šé“æ•°æ®")
        report.append("=" * 60)
        report.append("")

        from datetime import datetime
        report.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")

        report.append("## æ•°æ®æ‰©å¢ç­–ç•¥")
        report.append("æœ¬æŠ¥å‘ŠåŸºäºæ‰€æœ‰é€šé“æ•°æ®ï¼ˆDE + FE + BAï¼‰è®­ç»ƒçš„SCTL-FDæ¨¡å‹ï¼Œ")
        report.append("å®ç°äº†æ•°æ®é‡çš„3å€æ‰©å¢ï¼Œä»16kæ ·æœ¬å¢é•¿åˆ°49kæ ·æœ¬ï¼Œ")
        report.append("å¤šé€šé“ä¿¡æ¯èåˆæå‡äº†æ•…éšœè¯Šæ–­çš„é²æ£’æ€§å’Œå‡†ç¡®æ€§ã€‚")
        report.append("")

        report.append("## æ ¸å¿ƒä¼˜åŠ¿")
        report.append("### 1. æ•°æ®æ‰©å¢")
        report.append("- **DEé€šé“**: é©±åŠ¨ç«¯ä¿¡å·ï¼Œæ•…éšœç‰¹å¾æœ€ç›´æ¥æ¸…æ™°")
        report.append("- **FEé€šé“**: é£æ‰‡ç«¯ä¿¡å·ï¼Œæä¾›ä¸åŒè§’åº¦çš„æ•…éšœä¿¡æ¯")
        report.append("- **BAé€šé“**: åŸºåº§ä¿¡å·ï¼Œåæ˜ æ•´ä½“ç³»ç»ŸæŒ¯åŠ¨ç‰¹å¾")
        report.append("- **æ ·æœ¬å¢å¼º**: 49kæ ·æœ¬ vs 16kæ ·æœ¬ï¼Œç¼“è§£æ•°æ®ç¨€ç¼º")
        report.append("")

        report.append("### 2. ç‰¹å¾èåˆ")
        report.append("- **å¤šè§†è§’æ„ŸçŸ¥**: ä¸åŒä¼ æ„Ÿå™¨ä½ç½®çš„äº’è¡¥ä¿¡æ¯")
        report.append("- **é²æ£’æ€§å¢å¼º**: å•é€šé“æ•…éšœä¸å½±å“æ•´ä½“è¯Šæ–­")
        report.append("- **æ³›åŒ–èƒ½åŠ›**: æ›´ä¸°å¯Œçš„æ•°æ®åˆ†å¸ƒæå‡æ¨¡å‹æ³›åŒ–")
        report.append("")

        report.append("## åˆ†æç»´åº¦")
        report.append("### 1. äº‹å‰å¯è§£é‡Šæ€§")
        report.append("- æ¨¡å‹æ¶æ„é€æ˜æ€§åˆ†æ")
        report.append("- å¤šé€šé“æ•°æ®èåˆæœºåˆ¶")
        report.append("- å‚æ•°åˆ†å¸ƒç»Ÿè®¡")
        report.append("- è®¾è®¡ç†å¿µè¯´æ˜")
        report.append("")

        report.append("### 2. è¿ç§»è¿‡ç¨‹å¯è§£é‡Šæ€§")
        report.append("- ç‰¹å¾ç©ºé—´å˜åŒ–å¯è§†åŒ–ï¼ˆ3D t-SNEï¼‰")
        report.append("- ç±»é—´è·ç¦»åˆ†æ")
        report.append("- å¤šé€šé“ç‰¹å¾èåˆæ•ˆæœ")
        report.append("")

        report.append("### 3. äº‹åå¯è§£é‡Šæ€§")
        report.append("- åˆ†ç±»æ€§èƒ½è¯„ä¼°")
        report.append("- ç‰¹å¾é‡è¦æ€§åˆ†æ")
        report.append("- é”™è¯¯æ¡ˆä¾‹ç ”ç©¶")
        report.append("")

        report.append("## å…³é”®å‘ç°")
        report.append("### æ¨¡å‹ä¼˜åŠ¿")
        report.append("- âœ… æˆåŠŸå®ç°4åˆ†ç±»æ•…éšœè¯Šæ–­ï¼ˆåŒ…å«æ­£å¸¸çŠ¶æ€ï¼‰")
        report.append("- âœ… å¤šé€šé“æ•°æ®èåˆæå‡è¯Šæ–­å‡†ç¡®æ€§")
        report.append("- âœ… LSTMæ¶æ„é€‚åˆæ—¶åºæŒ¯åŠ¨ä¿¡å·å»ºæ¨¡")
        report.append("- âœ… ç›‘ç£å¯¹æ¯”å­¦ä¹ æä¾›åˆ¤åˆ«æ€§ç‰¹å¾è¡¨ç¤º")
        report.append("- âœ… æ•°æ®æ‰©å¢æœ‰æ•ˆç¼“è§£æ ·æœ¬ç¨€ç¼ºé—®é¢˜")
        report.append("")

        report.append("### å¯¹æ¯”ä¼˜åŠ¿ï¼ˆvs å•é€šé“ï¼‰")
        report.append("- ğŸš€ **æ•°æ®é‡**: 49k vs 16k ï¼ˆ3å€å¢é•¿ï¼‰")
        report.append("- ğŸš€ **ä¿¡æ¯ä¸°å¯Œåº¦**: ä¸‰é€šé“èåˆ vs å•é€šé“")
        report.append("- ğŸš€ **é²æ£’æ€§**: å¤šæºä¿¡æ¯ vs å•ä¸€ä¿¡æ¯æº")
        report.append("- ğŸš€ **æ³›åŒ–èƒ½åŠ›**: æ›´å…¨é¢çš„æ•…éšœæ¨¡å¼è¦†ç›–")
        report.append("")

        report.append("### æ”¹è¿›å»ºè®®")
        report.append("- ğŸ”„ å¯è€ƒè™‘æ³¨æ„åŠ›æœºåˆ¶ä¼˜åŒ–é€šé“æƒé‡")
        report.append("- ğŸ”„ å¯å°è¯•é€šé“çº§ç‰¹å¾èåˆç­–ç•¥")
        report.append("- ğŸ”„ å¯å¼•å…¥æ—¶é¢‘åŸŸè”åˆåˆ†æ")
        report.append("")

        report.append("## ç”Ÿæˆæ–‡ä»¶")
        report.append("- `plots/feature_transfer_analysis_all_channels_3d.png`: 3Dç‰¹å¾è¿ç§»å¯è§†åŒ–")
        report.append("- `plots/inter_class_distances_all_channels.png`: ç±»é—´è·ç¦»åˆ†æ")
        report.append("- `plots/confusion_matrix_all_channels.png`: æ··æ·†çŸ©é˜µ")
        report.append("- `plots/feature_importance_all_channels.png`: ç‰¹å¾é‡è¦æ€§")
        report.append("- `reports/architecture_analysis_all_channels.md`: æ¶æ„åˆ†æ")
        report.append("- `reports/classification_report_all_channels.txt`: åˆ†ç±»æ€§èƒ½æŠ¥å‘Š")
        report.append("- `reports/error_analysis_all_channels.txt`: é”™è¯¯åˆ†æ")
        report.append("")

        # ä¿å­˜ç»¼åˆæŠ¥å‘Š
        with open(os.path.join(self.report_dir, 'comprehensive_report_all_channels.md'), 'w', encoding='utf-8') as f:
            f.write('\n'.join(report))

        print("âœ“ ç»¼åˆå¯è§£é‡Šæ€§æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        print(f"ğŸ“Š æ‰€æœ‰åˆ†æç»“æœä¿å­˜åœ¨: {self.args.output_dir}")

    def run_analysis(self):
        """è¿è¡Œå®Œæ•´çš„å¯è§£é‡Šæ€§åˆ†æ"""
        print("å¼€å§‹SCTL-FDå¯è§£é‡Šæ€§åˆ†æï¼ˆæ‰€æœ‰é€šé“æ•°æ®ï¼‰...")

        # 1. åŠ è½½æ¨¡å‹å’Œæ•°æ®
        if not self.load_models():
            print("æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œç»ˆæ­¢åˆ†æ")
            return False

        if not self.load_data():
            print("æ•°æ®åŠ è½½å¤±è´¥ï¼Œç»ˆæ­¢åˆ†æ")
            return False

        # 2. æ‰§è¡Œå¤šç»´åº¦åˆ†æ
        self.analyze_model_architecture()      # äº‹å‰å¯è§£é‡Šæ€§
        self.analyze_feature_transfer()        # è¿ç§»è¿‡ç¨‹å¯è§£é‡Šæ€§
        self.analyze_decision_basis()          # äº‹åå¯è§£é‡Šæ€§
        self.generate_comprehensive_report()   # ç»¼åˆæŠ¥å‘Š

        print("\nğŸ‰ SCTL-FDå¯è§£é‡Šæ€§åˆ†æå®Œæˆï¼ˆæ‰€æœ‰é€šé“æ•°æ®ï¼‰ï¼")
        return True

def main():
    parser = argparse.ArgumentParser(description='SCTL-FDé«˜çº§å¯è§£é‡Šæ€§åˆ†æ - æ‰€æœ‰é€šé“æ•°æ®')

    # æ•°æ®è·¯å¾„
    parser.add_argument('--val_data_path', type=str,
                        default='processed_data_all_channels/source_val_all_channels.npz',
                        help='éªŒè¯æ•°æ®è·¯å¾„ï¼ˆæ‰€æœ‰é€šé“ï¼‰')

    # æ¨¡å‹è·¯å¾„
    parser.add_argument('--pretrained_encoder_path', type=str,
                        default='models_saved/lstm_supcon_all_channels/best_lstm_encoder.pth',
                        help='é¢„è®­ç»ƒencoderè·¯å¾„ï¼ˆæ‰€æœ‰é€šé“ï¼‰')
    parser.add_argument('--final_model_path', type=str,
                        default='models_saved/lstm_transfer_all_channels/best_lstm_transfer_model.pth',
                        help='æœ€ç»ˆæ¨¡å‹è·¯å¾„ï¼ˆæ‰€æœ‰é€šé“ï¼‰')

    # æ¨¡å‹å‚æ•°
    parser.add_argument('--signal_length', type=int, default=2048,
                        help='ä¿¡å·é•¿åº¦')
    parser.add_argument('--feature_dim', type=int, default=256,
                        help='ç‰¹å¾ç»´åº¦')
    parser.add_argument('--hidden_dim', type=int, default=128,
                        help='LSTMéšè—ç»´åº¦')
    parser.add_argument('--num_layers', type=int, default=2,
                        help='LSTMå±‚æ•°')
    parser.add_argument('--num_classes', type=int, default=4,
                        help='åˆ†ç±»ç±»åˆ«æ•°')

    # åˆ†æå‚æ•°
    parser.add_argument('--batch_size', type=int, default=32,
                        help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--output_dir', type=str, default='results/advanced_explainability_all_channels',
                        help='å¯è§£é‡Šæ€§åˆ†æç»“æœè¾“å‡ºç›®å½•ï¼ˆæ‰€æœ‰é€šé“ï¼‰')

    args = parser.parse_args()

    # åˆ›å»ºåˆ†æå™¨å¹¶è¿è¡Œ
    analyzer = AdvancedExplainabilityAnalyzerAllChannels(args)
    analyzer.run_analysis()

if __name__ == '__main__':
    main()