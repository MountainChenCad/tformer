"""
åŸºäºæ‰€æœ‰é€šé“(DE, FE, BA)çš„æ•°æ®é¢„å¤„ç†è„šæœ¬
æ‰©å¢æºåŸŸæ•°æ®é›†ï¼Œç”¨äºå¯¹æ¯”å­¦ä¹ è®­ç»ƒ
"""

import os
import glob
import numpy as np
import scipy.io as sio
from sklearn.preprocessing import RobustScaler
import warnings
warnings.filterwarnings('ignore')

def extract_fault_type_from_filename(filename):
    """ä»æ–‡ä»¶åæå–æ•…éšœç±»å‹"""
    base = os.path.basename(filename).lower()

    # æ­£å¸¸çŠ¶æ€
    if 'normal' in base or 'n_' in base:
        return 0, 'Normal'

    # å†…åœˆæ•…éšœ
    if base.startswith('ir') or 'inner' in base:
        return 1, 'Inner'

    # å¤–åœˆæ•…éšœ
    if base.startswith('or') or 'outer' in base:
        return 2, 'Outer'

    # æ»šåŠ¨ä½“æ•…éšœ
    if base.startswith('b') and ('007' in base or '014' in base or '021' in base or '028' in base):
        return 3, 'Ball'

    return -1, 'Unknown'

def load_matlab_file(filepath):
    """åŠ è½½MATLABæ–‡ä»¶å¹¶æå–æ‰€æœ‰é€šé“æ•°æ®"""
    try:
        mat_data = sio.loadmat(filepath)

        # è·å–æ‰€æœ‰å˜é‡å
        var_names = [k for k in mat_data.keys() if not k.startswith('__')]

        # å¯»æ‰¾DE, FE, BAæ•°æ®
        de_data = None
        fe_data = None
        ba_data = None

        for var_name in var_names:
            var_lower = var_name.lower()
            if 'de_time' in var_lower or (var_name.endswith('DE_time')):
                de_data = mat_data[var_name].flatten()
            elif 'fe_time' in var_lower or (var_name.endswith('FE_time')):
                fe_data = mat_data[var_name].flatten()
            elif 'ba_time' in var_lower or (var_name.endswith('BA_time')):
                ba_data = mat_data[var_name].flatten()

        return de_data, fe_data, ba_data

    except Exception as e:
        print(f"Warning: Failed to load {filepath}: {e}")
        return None, None, None

def segment_signal(signal, segment_length=2048, overlap_ratio=0.5):
    """å°†ä¿¡å·åˆ†æ®µï¼Œæ”¯æŒ50%é‡å """
    if signal is None or len(signal) < segment_length:
        return []

    stride = int(segment_length * (1 - overlap_ratio))
    segments = []

    for i in range(0, len(signal) - segment_length + 1, stride):
        segment = signal[i:i + segment_length]
        segments.append(segment)

    return segments

def process_all_channels_data(data_dir, output_dir, segment_length=2048):
    """å¤„ç†æ‰€æœ‰é€šé“æ•°æ®ï¼Œåˆ›å»ºæ‰©å¢æ•°æ®é›†"""
    print(f"Processing data from: {data_dir}")

    # å¯»æ‰¾æ‰€æœ‰.matæ–‡ä»¶
    mat_files = []
    for root, dirs, files in os.walk(data_dir):
        for file in files:
            if file.endswith('.mat'):
                mat_files.append(os.path.join(root, file))

    print(f"Found {len(mat_files)} .mat files")

    all_samples = []
    all_labels = []
    label_counts = {0: 0, 1: 0, 2: 0, 3: 0}
    file_info = []

    for mat_file in mat_files:
        # æå–æ•…éšœç±»å‹
        label, fault_name = extract_fault_type_from_filename(mat_file)
        if label == -1:
            print(f"Skipping unknown file: {mat_file}")
            continue

        # åŠ è½½æ•°æ®
        de_data, fe_data, ba_data = load_matlab_file(mat_file)

        # å¤„ç†æ¯ä¸ªé€šé“
        channels = [('DE', de_data), ('FE', fe_data), ('BA', ba_data)]

        for channel_name, channel_data in channels:
            if channel_data is None:
                continue

            # åˆ†æ®µå¤„ç†
            segments = segment_signal(channel_data, segment_length)

            if len(segments) > 0:
                print(f"  {os.path.basename(mat_file)} - {channel_name}: {len(segments)} segments, Label: {fault_name}")

                for seg_idx, segment in enumerate(segments):
                    # è´¨é‡æ£€æŸ¥ï¼šå»é™¤å…¨é›¶æˆ–å¼‚å¸¸ç‰‡æ®µ
                    if np.std(segment) < 1e-6 or np.any(np.isnan(segment)) or np.any(np.isinf(segment)):
                        continue

                    # é²æ£’æ ‡å‡†åŒ–
                    scaler = RobustScaler()
                    normalized_segment = scaler.fit_transform(segment.reshape(-1, 1)).flatten()

                    all_samples.append(normalized_segment)
                    all_labels.append(label)
                    label_counts[label] += 1

                    file_info.append({
                        'file': os.path.basename(mat_file),
                        'channel': channel_name,
                        'segment': seg_idx,
                        'label': label,
                        'fault_name': fault_name
                    })

    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    samples_array = np.array(all_samples)
    labels_array = np.array(all_labels)

    # æ•°æ®é›†ç»Ÿè®¡
    print(f"\n=== æ•°æ®é›†ç»Ÿè®¡ ===")
    print(f"æ€»æ ·æœ¬æ•°: {len(samples_array)}")
    print(f"ä¿¡å·é•¿åº¦: {segment_length}")
    print(f"æ ‡ç­¾åˆ†å¸ƒ:")
    fault_names = ['Normal', 'Inner', 'Outer', 'Ball']
    for i, name in enumerate(fault_names):
        print(f"  {name}: {label_counts[i]} æ ·æœ¬ ({label_counts[i]/len(samples_array)*100:.1f}%)")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)

    # æ•°æ®åˆ†å‰²ï¼š80%è®­ç»ƒï¼Œ20%éªŒè¯
    from sklearn.model_selection import train_test_split

    train_data, val_data, train_labels, val_labels = train_test_split(
        samples_array, labels_array,
        test_size=0.2,
        stratify=labels_array,
        random_state=42
    )

    # ä¿å­˜è®­ç»ƒé›†
    train_path = os.path.join(output_dir, 'source_train_all_channels.npz')
    np.savez_compressed(train_path, samples=train_data, labels=train_labels)
    print(f"è®­ç»ƒé›†ä¿å­˜åˆ°: {train_path}")
    print(f"è®­ç»ƒæ ·æœ¬: {len(train_data)}")

    # ä¿å­˜éªŒè¯é›†
    val_path = os.path.join(output_dir, 'source_val_all_channels.npz')
    np.savez_compressed(val_path, samples=val_data, labels=val_labels)
    print(f"éªŒè¯é›†ä¿å­˜åˆ°: {val_path}")
    print(f"éªŒè¯æ ·æœ¬: {len(val_data)}")

    # ä¿å­˜å®Œæ•´æ•°æ®é›†
    all_path = os.path.join(output_dir, 'source_all_all_channels.npz')
    np.savez_compressed(all_path, samples=samples_array, labels=labels_array)
    print(f"å®Œæ•´æ•°æ®é›†ä¿å­˜åˆ°: {all_path}")

    # ä¿å­˜å¤„ç†ä¿¡æ¯
    import json
    info_dict = {
        'total_samples': len(samples_array),
        'segment_length': segment_length,
        'overlap_ratio': 0.5,
        'label_distribution': label_counts,
        'train_samples': len(train_data),
        'val_samples': len(val_data),
        'channels_used': ['DE', 'FE', 'BA'],
        'normalization': 'RobustScaler',
        'file_count': len(mat_files),
        'fault_names': fault_names
    }

    info_path = os.path.join(output_dir, 'processing_info_all_channels.json')
    with open(info_path, 'w', encoding='utf-8') as f:
        json.dump(info_dict, f, indent=2, ensure_ascii=False)
    print(f"å¤„ç†ä¿¡æ¯ä¿å­˜åˆ°: {info_path}")

    return train_path, val_path, all_path

def main():
    # æ•°æ®è·¯å¾„é…ç½®
    source_data_dir = '../data/æºåŸŸæ•°æ®é›†'
    output_dir = '../processed_data_all_channels'

    print("=== åŸºäºæ‰€æœ‰é€šé“çš„æ•°æ®é¢„å¤„ç† ===")
    print("å¤„ç†DE, FE, BAä¸‰ä¸ªé€šé“æ•°æ®ï¼Œæ‰©å¢æºåŸŸæ•°æ®é›†")

    # æ£€æŸ¥æ•°æ®ç›®å½•
    if not os.path.exists(source_data_dir):
        print(f"Error: æºæ•°æ®ç›®å½•ä¸å­˜åœ¨: {source_data_dir}")
        print("è¯·ç¡®ä¿å·²ä¸‹è½½å¹¶è§£å‹æºåŸŸæ•°æ®é›†")
        return

    # å¤„ç†æ•°æ®
    try:
        train_path, val_path, all_path = process_all_channels_data(
            source_data_dir, output_dir
        )

        print(f"\nâœ… æ‰€æœ‰é€šé“æ•°æ®é¢„å¤„ç†å®Œæˆ!")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        print(f"ğŸ“Š å¯ç”¨äºç›‘ç£å¯¹æ¯”å­¦ä¹ è®­ç»ƒ")

    except Exception as e:
        print(f"âŒ æ•°æ®å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()